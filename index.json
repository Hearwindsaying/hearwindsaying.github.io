[{"authors":null,"categories":null,"content":"I am a master student in computer science at Dartmouth College.\nI am strongly interested in computer graphics, especially in physically based rendering. I attempt to learn and understand the principles behind natural phenomena appearing in the physical world. I could then synthesize realistic images in a virtual world fabricated by math and code.\nPreviously I worked on analytical area light integration for interests under the supervision of Dr. Li-Yi Wei. My research interests involve efficient sampling and integration for rendering application.\n  Download my resumé and checkout my portfolio.\n","date":1598918400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1598918400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a master student in computer science at Dartmouth College.\nI am strongly interested in computer graphics, especially in physically based rendering. I attempt to learn and understand the principles behind natural phenomena appearing in the physical world.","tags":null,"title":"Zihong Zhou","type":"authors"},{"authors":["Zihong Zhou and Yunquan Gu"],"categories":[],"content":"","date":1670803200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670803200,"objectID":"7251b9724a59505c035ac294dfa248c9","permalink":"https://hearwindsaying.github.io/project/rendering-competition/","publishdate":"2022-12-12T00:00:00Z","relpermalink":"/project/rendering-competition/","section":"project","summary":"Final report of the rendering competition for CS87/287: Rendering Algorithms at Dartmouth College. **We won the grand prize!**","tags":["Physically based rendering","Final project"],"title":"Rendering Competition FA22","type":"project"},{"authors":[],"categories":["GPU Ray Tracing","pbrt-v4"],"content":"","date":1652140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652140800,"objectID":"569fc2d9f126f3c3558d76347deaa766","permalink":"https://hearwindsaying.github.io/post/pbrt-v4-talk/","publishdate":"2022-05-10T00:00:00Z","relpermalink":"/post/pbrt-v4-talk/","section":"post","summary":"A talk given at Revobit on studying pbrt-v4's architecture for GPU ray tracing.","tags":["pbrt-v4","GPU Ray Tracing"],"title":"An Engineer's Look on the Architecture of pbrt-v4 for GPU Ray Tracing","type":"post"},{"authors":[],"categories":["Rendering"],"content":"Overview I spent a few time in 2021 learning realtime rendering techniques and graphics API such as OpenGL and DirectX 12. I then immediately turn to DirectX Raytracing (DXR) as a big fan of tracing rays. So this is yet another (the second) global illumination renderer I have written so far. Though it is quite similar to my first ray tracer Colvillea, it is based on the graphics API (DXR) as opposed to GPGPU one (like CUDA or OpenCL).\nSome Sneaky Stuff on the Road I think I might have run into some issues during the journey and doing some renderings again is indeed a bit more difficult than before. Well, I have learned ray tracing before and related algorithms will not change if I switch to another way of implementation \u0026ndash; no matter I write in plain C/old school C++/modern C++/CUDA/HLSL, the math and physics behind the rendering are essentially the same. However, the coding process could suffer:\n A possible two-sided BRDF adapter implementation in HLSL-2021 (left) and C++11 (right). The C++ code is from Mitsuba 0.5.   For example, it is useful to support two-sided shading for the ray tracer. Since common 3d modeling packages such as Maya and Blender usually enable two-sided shading by default, artist may ignore the orientation of the surface during the modeling. If we look at a surface from backside, we could still receive the lighting reflected from this object. But checking the side orientation inside the implementation of BRDF classes is bothersome and inspired by Mitsuba, we have an elegant approach for this. The idea is to add a new so-called two-sided BRDF to wrap the one-sided BRDF \u0026ndash; we keep the implementation as-is in one-sided BRDF (do the shading as long as the incoming ray and the outgoing ray are in the same hemisphere as the surface\u0026rsquo;s normal) and flip the ray direction in two-sided BRDF when necessary. You could have a look at the Figure 1 if you are not sure about what I mean.\nWell the coding in C++/C/C#\u0026hellip; is pretty straightforward due to the language support for class, member functions, function pointers (and even templates for C++ gurus). However, in HLSL, none of these great features exists and I believe it\u0026rsquo;s even worse in GLSL. The good news is that to avoid replicating the two-sided BRDF code for each BRDF type, we could use macro\u0026hellip; But the bad news is HLSL\u0026rsquo;s preprocessor are not the same as C++/C\u0026rsquo;s preprocessor, which supports variadic macros for example. I thus have to manually emulate the variadic behavior and arguments forwarding (compared to modern C++, we could use variadic templates and perfect forwarding). A much more elegant approach to deal with this is introducing another abstraction:\n Computer Science is a science of abstraction -creating the right model for a problem and devising the appropriate mechanizable techniques to solve it. \u0026ndash;Alfred Aho\n Fortunately, Slang does this for us \u0026ndash; a shading language enables us to write shaders efficiently with modern language features such as interfaces and generics and we could safely rely on Slang compiler to generate high performance backend code (e.g. HLSL). This seems to be a promising improvement worthy of trying out in the future.\nSample Renderings Results One advantage of writing ray tracers over rasterization is that we could generate some pretty images easily, and perhaps efficiently as well if we build atop RTX. Once we implement a simple integrator such as naive path tracing integrator with a decent BSDF such as Disney Principle BSDF and with some nice area lighting, appealing effects such as reflection and color bleeding are achieved. In realtime rendering however, the world is fragmented \u0026ndash; we have to deal with shadows, reflections, GIs etc. separately. Have a look at my blog post living room in unity for example, which tries to match the nice result from a simple ray tracer Colvillea using a world-leading game engine.\nSo I can\u0026rsquo;t wait to quickly implement all these simple components in ray tracing and support GLTF 2.0 (featuring with PBR materials) parsing, in which way I could simply export nice scene by artists from Blender and import to the path tracer.\nHere is how it looks like from the renderer:\n The chessboard scene rendered with 8192spp in 8min45s. Runs in 18fps or 55 ms for 1 iteration (1spp) on a RTX 2060 (minimum GPU requirements for running DXR API). Scene credits: https://davzeppelin.gumroad.com/l/QlHKc.   Accidentally, I have set Fresnel to 1.0 for specular lobe in Disney BRDF for debugging purpose and just forgot to turn off before rendering the Chess scene and got a funny bug:  An interesting result due to a bug.   Besides its over brightness, featuring strong specular reflections and blurring out diffuse details, because of the wrong shading I cannot tell the opponent\u0026rsquo;s chess pieces from ours! :) I like rendering since sometimes I get these magical images for my bugs (instead of getting crashes and nothing in other applications).\nThere is another scene with more complicated geometries:\n The living room rendered with 8192spp in 19min33s. Runs in 10fps or 100 ms for 1 iteration (1spp) for this scene. Scene credits: https://www.youtube.com/watch?v=Gn1biEB5PbQ\u0026amp;t=44s.   Disney BRDF is used for all materials in the two scenes and several hidden quadlights are used as light sources. I also used Dr. Laurent Belcour\u0026rsquo;s Screen Space Blue Noise Error Diffusion Sampler with XOR Scrambled Sobol sequence for high quality samples pattern. Finally, Intel\u0026rsquo;s OpenImageDenoiser is used for cleaning up noises left in low sample counts.\nMachine Learning Based Denoiser \u0026ndash; A Savior Path tracing integrator is easy to code, but it still requires tremendous effort to reach a fully converged image without noises. There are quite a few denoising algorithms and open source implementations available. Check out Alain Galvan\u0026rsquo;s blog for a good overview of ways to denoise a path tracer. In a nutshell, he classifies the denoising arts into sampling based (sophisticated filters such as SVGF) and machine learning based method. NVIDIA\u0026rsquo;s OptiX Denoiser and Intel\u0026rsquo;s OpenImageDenoiser (OIDN) are two representatives for machine learning denoisers, which is usually shipped with production renderers like V-Ray and Blender Cycles. Due to its simplicity for integration, I started with OIDN.\nOIDN is built and trained with Convolutional Neural Network (CNN). Outside the blackbox, it just works like a filter so it could be put into the postprocessing stage. Feed the OIDN with noisy ray tracing outputs and auxiliary buffers (albedo, normal etc.) from our AOV outputs, it gives us a noise free result in a short time. Well I mean a short time but it still needs some noticeable time \u0026ndash; 2s for 2K resolution on my i7-10700K CPU.\nThis DXR ray tracer is designed to be interactive and progressive, but 2s could really block the main/UI thread and make it unresponsive. So I move the denoising pass to the background thread and run it from time to time asynchronously. The idea is to progressively accumulate a few samples, say 64 from a well-designed Sobol sampler, and run the denoiser in the background. Once it is done, show this denoised image in the viewport and keep ray tracing. After running for a while, run the denoiser again and update the denoised image. This makes sense since we cannot easily spot the difference between denoising a 64spp input and 65spp input, but we do for the 64spp input and 90spp input. I find this approach works well in practice, trying to keep CPU and GPU busy and the user\u0026rsquo;s interaction still goes smoothly:\n  Here are some more comparisons (drag the slide in the middle left for comparison):\n100spp Ray Tracing (84.36s)\r\r100spp Denoised (84.36s)\r\r\r\r1024spp Ray Tracing (136.30s)\r\r1024spp Denoised (136.30s)\r\r\r\r Comparisons between ray traced, denoised, referenced images.   And a more easily noticeable verification is using ImageDiff to the ground truth:\n\r\r\r\rMachine Learning Based Denoiser \u0026ndash; A Savior If With Our Help I am kind of appalled by the advertisements of Intel\u0026rsquo;s OIDN gallery and after trying out myself, it looks like a lifesaver to us! Unfortunately, this is not the whole story. I cannot just forget all the sophisticated algorithms invented by diligent graphics researchers and solely focus on hoping ML denoiser improve my renderings given some \u0026lsquo;arbitrary\u0026rsquo; inputs. For example, if I turn off multiple importance sampling and use light sampling only:\n  As is shown in the video, weird structural pattern can be seen. It is animated since I continuously feed the denoiser with more samples input and it tries very hard to improve the result\u0026hellip;\nAnd with our help to reduce variance at first by using multiple importance sampling:\n  The roughness is set to be low and we will have a peak specular lobe, in which case light sampling cannot help too much but BSDF sampling does, resulting far fewer noises and the denoiser works pretty well now. As pointed out by Bitterli et al., currently these denoisers are not able to bring in the features which are not present in input samples. So we need to keep finding out new strategies for sampling, image reconstruction etc. to help our denoisers out. I suppose this is why though lots of people turn to deep learning recently, there are still appealing research on traditional rendering.\n","date":1635429912,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635429912,"objectID":"0266c95b925f781502c23f757eae413a","permalink":"https://hearwindsaying.github.io/post/sample-results-from-an-anonymous-dxr-renderer/","publishdate":"2021-10-28T22:05:12+08:00","relpermalink":"/post/sample-results-from-an-anonymous-dxr-renderer/","section":"post","summary":"Overview I spent a few time in 2021 learning realtime rendering techniques and graphics API such as OpenGL and DirectX 12. I then immediately turn to DirectX Raytracing (DXR) as a big fan of tracing rays.","tags":[],"title":"Sample Results From an Anonymous DXR Renderer","type":"post"},{"authors":["Zihong Zhou","Li-Yi Wei"],"categories":null,"content":"Summary We present triple sphere, a method to integrate spherical lights over spherical caps via spherical harmonics for rendering applications.\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"53e70e40936470c23eef90852de6774e","permalink":"https://hearwindsaying.github.io/publication/triplesphere/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/publication/triplesphere/","section":"publication","summary":"In *SIGGRAPH Asia 2020 Technical Communications*","tags":[],"title":"Spherical Light Integration over Spherical Caps via Spherical Harmonics","type":"publication"},{"authors":null,"categories":null,"content":"Here are some of selected shots from Prof. Lorie Loeb\u0026rsquo;s modeling class1 I took in Fall 22. Interestingly, I also took Prof. Wojciech Jarosz\u0026rsquo;s rendering algorithms in FA22 and they are somewhat complementary to each other. Rendering Algorithms tells us how to render photorealistic images given the scene and Digital Modeling tells us how to author the scene for a renderer and use the renderer from an artistic perspective.\nClassroom  Motivation image from artstation. Credits: https://www.artstation.com/artwork/L20Kbl    Attempts at reproducing the motivation classroom, with some customization.    Best shot I love. CS87/287 Rendering Algorithms is so hard!    Bring pbrt into live!    Looking back at the classroom.    Scratch with some volumetrics rendering.   Abstract  Portal in Big Hero 6. Image credits: http://alexey.stomakhin.com/research/portal.html   My abstract design is inspired by a talk at SIGGRAPH 2015 \u0026ndash; Big Hero 6: Into the Portal. They author this portal using a variation of Mandelbrot fractals with volumetric path tracing. I went in another direction instead by applying deformers to geometry.\n My portal design WIP. Rendered with Arnold for Maya 2022    Final rendering for my abstract.   Credits Modeling, UV layout and rendering are done by myself. Textures are external resources:\nMathematicians:\n https://www.thefamouspeople.com/profiles/archimedes-422.php https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss#/media/File:Carl_Friedrich_Gauss_1840_by_Jensen.jpg https://en.wikipedia.org/wiki/Leonhard_Euler#/media/File:Leonhard_Euler_-_edit1.jpg https://www.usna.edu/Users/math/meh/riemann.html http://scihi.org/wp-content/uploads/2014/04/Henri_Poincare2.jpg http://scihi.org/henri-poincare/ https://en.wikipedia.org/wiki/Joseph-Louis_Lagrange#/media/File:Лагранж.jpg https://www.sapaviva.com/euclid-of-alexandria/ https://en.wikipedia.org/wiki/David_Hilbert https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz#/media/File:Christoph_Bernhard_Francke_-_Bildnis_des_Philosophen_Leibniz_(ca._1695).jpg https://upload.wikimedia.org/wikipedia/commons/e/ef/Alexander_Grothendieck.jpg https://upload.wikimedia.org/wikipedia/commons/f/f3/Pierre_de_Fermat.jpg https://upload.wikimedia.org/wikipedia/commons/5/53/Evariste_galois.jpg https://www.daviddarling.info/images/Riemann.jpg https://www.mapsofindia.com/world-map/world-political-map-2020.jpg?v:1.0  Others:\n Chalkboard: https://www.freepik.com/free-photo/old-black-background-grunge-texture-dark-wallpaper-blackboard-chalkboard-concrete_20038789.htm#query=chalkboard%20texture\u0026position=3\u0026from_view=keyword Wall hangings are from my portfolio: https://hearwindsaying.github.io/portfolioWeb/index.html Grid texture and pbrt book cover textures are from pbrt-v2 (miscquads.pbrt）: https://www.pbrt.org/scenes-v2 MDLHandbook picture is from NVIDIA MDL Handbook: http://mdlhandbook.com/ Several book pages are from pbrt-v3: https://pbr-book.org/ HDRI are from: https://polyhaven.com/a/garden_nook Quiz pages are from CS87/287 Rendering Algorithms Quiz Source images for wine bottle: https://www.youtube.com/watch?v=4R0e6O5afE4    I have two shots, room and abstract rendering featured on the course website (Winter 23 version)!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"dfa5a142f99b7bb3cfa15545ef2357d9","permalink":"https://hearwindsaying.github.io/project/cs22-modeling/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/project/cs22-modeling/","section":"project","summary":"Projects for CS22/122: 3D Digital Modeling.","tags":["Rendering","Digital Arts","Final Project"],"title":"3D Digital Modeling FA22","type":"project"},{"authors":null,"categories":null,"content":"Overview Colvillea is a physically based global illumination renderer running on GPU. It relies on NVIDIA\u0026rsquo;s OptiX to achieve parallelism by leveraging GPU resources, resulting in high performance ray tracing rendering.\n This project is archived and I am working on a new version of Colvillea that has much better performance thanks to OptiX 7 and wavefront path tracing.   Motivation Here are some motivations and objectives of building Colvillea:\n Ease for implementation of ray tracer in GPU. Writing a GPU renderer from scratch could be of great difficulty and hard to get optimal performance. Debugging is also a pain in the neck. There might be a way out for all these problems thanks to OptiX. Deliver RTX hardware acceleration for faster rendering. OptiX is one of the three ways for enabling RTCores so as to achieve higher ray tracing efficiency when possible. Potential for implementation of some state-of-the-art rendering technologies. This is a personal project written during my learning of computer graphics. In the end, it should be both easy and convenient to extend to adding more features. It\u0026rsquo;s also interesting to try out rendering algorithms in GPU to explore a better efficiency.  Features Light Transport  Direct Lighting Unidirectional Path Tracing  Reflection Models  Lambertian BRDF Specular BRDF (Perfect Mirror) Specular BSDF (Perfect Glass) Ashikhmin-Shirley BRDF (Rough Plastic) GGX Microfacet BRDF (Rough Conductor) GGX Microfacet BSDF (Rough Dielectric) Dielectric-Couductor Two Layered BSDF  Sampler  Independent Sampler Halton QMC Sampler (Fast Random Permutation) Sobol QMC Sampler  Filter  Box filter Gaussian filter  Rendering Mode  Progressive Rendering  Light Source Models  Point Light Quad Light (Spherical Rectangular Sampling) Image-Based Lighting (HDRI Probe)  Camera  Pinhole Camera Depth of Field  Geometry  Triangle Mesh (Wavefront OBJ)  Miscellaneous  LDR/HDR Image I/O with Gamma Correction Interactive rendering with editing scene  Build Building Colvillea requires OptiX 6.0 (6.5 is preferred) and CUDA 9.0 or above installed. For graphics driver on Windows platform, driver version 436.02 or later is required. All NVIDIA GPUs of Compute Capability 5.0 (Maxwell) or higher are supported but those with Turing architecture is required to access RTX hardware acceleration.\nColvillea currently builds on Windows only using CMake and could be built using MSVC successfully. It\u0026rsquo;s recommeded that create a separte directory in the same level folder as src folder. Note that you are required to use VS2015 or above targeted for 64-bit as CUDA_HOST_COMPILER in configuration step. For better layout to support interactive rendering, please put imgui.ini file to the same directory as colvillea.vcxproj.\nSelected Images            References   NVIDIA OptiX\n  PBRT\n  Mitsuba\n  ","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://hearwindsaying.github.io/project/example/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"A Physically Based GPU Ray Tracer","tags":["Ray Tracing","GPU","Physically Based Rendering"],"title":"Colvillea","type":"project"},{"authors":[],"categories":[],"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"431c0f3f6aea07e8fa8c5445342b5bd7","permalink":"https://hearwindsaying.github.io/project/livingroom-in-unity/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/project/livingroom-in-unity/","section":"project","summary":"A course project on Virtual Reality exploring Lightmapping techniques with the  High-Definition Rendering Pipeline in Unity.","tags":["Unity","Prebaked Ray Tracing","Digital Art"],"title":"Living-Room in Unity","type":"project"},{"authors":[],"categories":[],"content":"Overview This is our final assignment for the \u0026lsquo;Object Oriented Programming\u0026rsquo; course. It is a UWP (Universal Windows Platform) application for simple image viewing written in C++17/WinRT and XAML. Except for the UI layout and design by @ArtlexKylin, I did all the implementation and optimization.\nFeatures  Common image format supports: png, jpg, git, bmp, tif, tga. Directory tree support for UWP and folder import support (for security purpose)1. Image file manipulations.  Selective: single/multiple/invert selections, click and drag selection (dragable rectangle selection)2. File system: copy, cut, move, paste, remove, rename and rename in batch. Efficient image searching with suggestions. Simple image viewing and edits: scaling, rotations, filters (Gaussian Blur, Exposure, Temperature etc.).   Image gallery viewing.  Automatic slide show images. Efficient thumbnails and full images showing.   Fluent UI design with acrylic material and adaptive layout powered by WinUI 2.0. ListView and GridView optimizations for thousands of items.  UI virtualization and update items on demand. Native C++17 implementation with XAML data binding support.    Build Currently it supports from Windows 10 version 1809 (OS build 17763) and a Windows SDK \u0026gt;= 10.0.17763.0 is required. Both Visual Studio 2017 (v141) and 2019 (v142) could be used and Visual Studio\u0026rsquo;s retarget solution should provide you with the available SDK/project verisons. It depends on Microsoft.UI.Xaml, Microsoft.Windows.CppWinRT and Win2D.uwp by nuget packages.\nGallery   Thumbnail and treeview for images. Draggable rectangle selection in UWP.     Gallery view for images.     Filters preview.     UWP app is notable at its security and safeness. In other words, it cannot access extra resources not authorized by users. So you could import the folder explicitly to give the accessibility or just turn on privacy settings for UWP app in windows settings to enable the directory tree working. Please see also https://support.microsoft.com/en-us/windows/-windows-file-system-access-and-privacy-a7d90b20-b252-0e7b-6a29-a3a688e5c7be for instructions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Note that dragable rectangle selection is common in traditional win32 application (e.g. explorer.exe), but it is not compatible with fluent UI design. I still implemented this widget from scratch due to its functional convenience and assignment requirement.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1552176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552176000,"objectID":"5cf7a14226181e96d6cd8e73955e98fa","permalink":"https://hearwindsaying.github.io/project/simple-photo-viewer/","publishdate":"2019-03-10T00:00:00Z","relpermalink":"/project/simple-photo-viewer/","section":"project","summary":"A simple photoviewer in C++/WinRT deployed at Universal Windows Platform.","tags":[],"title":"Simple Photo Viewer","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne \r**Two** \rThree \r A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://hearwindsaying.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["Unity","Rendering","Digital Arts"],"content":"Notes This is my final project for a virtual reality and game development course. My contribution is bringing a Benedikt Bitterli\u0026rsquo;s famous rendering scene living-room to Unity and showing how we could use various rasterization techniques to approximate the beauty image produced by offline renderer.\nForewords Ray tracing is an elegant algorithm used in computer graphics to synthesis realistic images. It shoots tremendous rays to find intersection with geometries in the scene and shading with lights and tries its best to simulate the light transport process in nature. The algorithm is not so fast however and it is not unusual to spend hours to days rendering a single image for the complex scenes produced by artists in the movie industry.\nRasterization on the other hand, represents a way to render efficiently, which is used ubiquitously in realtime rendering. Thanks to the efforts of many graphics researchers and engineers for many years, realtime rendering could already produce some good-looking images nowadays. Consequently, in this project I\u0026rsquo;d like to find out how good it could be in a game engine.\nNote that these days realtime ray tracing becomes popular due to the introduction of NVIDIA\u0026rsquo;s RTX architecture in Turing GPUs, which is able to do the ray intersection in a lightning fast way. However, it requires a decent graphics card and my old GTX-960M is a bit out-of-date. So I give up the idea and turn to Unity\u0026rsquo;s HDRP Rendering Pipeline (The High Definition Render Pipeline) for the project.\nGeneral To generate an image as realistic as possible, we need to choose a GI (Global Illumination) system in Unity. We will miss the nice indirect lighting, reflections for example if we skip this part.\n   Global Illumination Off Global Illumination On          Unity engine has several built-in solutions for achieving Global Illumination:   Two global illumination systems in Unity engine. Image courtesy of Unity documentation.    Update in 2021: Note that Enlighten is deprecated and we are suggested to use CPU or GPU-based Progressive Lightmapper instead.   I attempted to use Enlighten in the first place and do not have a good experience since it is laborious to play with the parameters to reach my expectation. I then turned to CPU and GPU based Progressive Lightmapper for the job. Progressiveness indicates we could have a somewhat coarse preview immediately and it gradually gets refined as we wait. This is usually preferred by lots of artists since it saves time for fast iterations.\nSky and Fog Volume To begin with, we should have a sky!\n  Our sunset skybox for the living room.   I chose to use a sky from HDRI probe, which would also be the primary luminary for our scene. We just need to import our HDRI panorama as a cubemap by setting the Texture Shape as Cube and using Trilinear Filter:\n  Import our skybox with cubemap and trilinear filter.   We could preview our loaded HDRI Sky in the scene as well by setting up the Volume:\n  Set up the HDRI Sky in Volume.   Remember we need to specify the sky in the Lighting/Environment HDR.\n  Specify our sky in Lighting.   Cleanup the Scene The living-room scene is from Benedikt Bitterli\u0026rsquo;s Rendering Resources:\n  The living room I chose.   I first import all meshes (*.obj) into Maya and group the objects by materials according to the provided scene description file (.xml). This could reduce our time for authoring and assigning materials in Unity later.\n  Group and name objects by materials in Maya.   I also check the normals facing outwards and ensure the Y axis is up (the convention Unity uses):\n  Visualize the normal to check the face orientation.   Import the Scene After fixing all issues in the last section, it should work well to export the scene as FBX and import in Unity.\nHowever, there is an issue to be considered \u0026ndash; what is our scale of the scene? This is important since we have limited precision for representing decimal in computer science. For example, the intersection point may slightly below the surface and when it shoots a shadow ray to the light to find out whether it\u0026rsquo;s blocked or not, it will be occluded by itself \u0026ndash; a phenomenon known as shadow acne.\n  Shadow acne artifacts due to the error of floating point. Image courtesy of Scratchpixel 2.0.   An idea is to offset the intersection point by epsilon, which should be related to the scale of the scene. If our scene lives in a small world, \u0026lsquo;a miss (ray epsilon in this context) is as good as a mile\u0026rsquo;! Our scale or units thus should choose consistently throughout the scene to avoid these annoying artifacts.\nOne of the simplest solutions is to compare the imported scene with the default primitive in Unity. A standard cube here has the length of 1 meter and I just scale our scene to make it match the scale \u0026ndash; a 5m*10m*5m living room.\n  Scale our scene according to the standard cube in Unity to fix the unit.   Testing Global Illumination After importing the models, it is time for playing around with the global illumination system now! We could first ignore the minute props in the scene such as plants and the statue and check whether the global illumination works well for the \u0026lsquo;global\u0026rsquo; environment, the look of the house. So enable the paneling, ceiling, floor, wall and the fireplace glass only and tick Contribute Global Illumination under Inspector/Mesh Renderer/Lighting.\n  Parameters required to contribute global illumination.   And we could enable the baked GI solution under Lighting/Mixed Lighting and Lighting/Lightmapping Settings.\n  Settings for lightmap baking.   Progressive GPU lightmapper is preferred when we have enough GPU memory (at least 4GB), which is generally the way much faster than the CPU one. It requires us to have a GPU supporting OpenCL 2.2 as well.\nMultiple importance sampling should always be enabled since it helps reduce variance by combining several sampling strategies. Throwing more direct and indirect samples helps reduce noises even more, but it increases the baking time as well. So we have to make a trade-off between the quality and time. But that\u0026rsquo;s not the whole story, since we could apply filtering even if we use fewer number of samples to get a decent result. By selecting the Auto setting for the Filtering, unity will do all the magic for us!\nSince we are using baked GI solution, so we must have a second UV for lightmapping. This is done by Unity by enabling Generate Lightmap UVs during FBX import.\n  Let Unity generate the lightmap UVs for us.   Material and Reflections Since we have already grouped the meshes by materials, we could just assign each group of mesh with a unique material. For the ceiling and the wall, we want a Lambertian BSDF as specified in the xml file. I just use a simple HDRP/Lit shader with Metallic = 0 and Smoothness = 0.\n   Material Base Color     ceiling R=G=B=0.578596   wall R=G=B=0.4528      Setup for our ceiling material to match a Lambertian BSDF.   For the floor and paneling, we need to emulate the substrate BSDF or rough-coating BSDF with the underlying BSDF being Lambertian and coated with a rough dielectric BSDF with GGX distribution.\n  Illustration for light transport in a rough-coating BSDF. Image courtesy of Mitsuba 0.5 documentation.   Thanks to the Laurent Belcour\u0026rsquo;s excellent work on layered material, we could easily simulate this kind of coating material in Unity using StackLit shader.\nWe shall use the Shader Graph to finish the task. Here is what we have for the floor:\n  Shader Graph setup for the layered floor material.   And the paneling material:\n  Shader Graph setup for the paneling.   The last material for our \u0026ldquo;global\u0026rdquo; environment is the fireplace, which is a perfectly smooth specular shader. Besides setting the Smoothness = 1 with StackLit, we need to deal with the reflection. This is one of the nasty stuff in realtime rendering \u0026ndash; though reflection itself is a part of the global illumination, we still need to explicitly achieve the effect by using another technique \u0026ndash; Reflection Probes. Here we create a reflection probe to capture the interior environment for the fireplace. Clicking Bake option under the Lighting, we get the nice reflection for the fireplace!\n  Fix the reflection of our fireplace.   But not for the interior, oops!\n  A weird look of our interior.   It may not be so obvious, but the interior gets reflection from the exterior HDRI or Skybox instead of the interior \u0026ndash; the floor is not missing, but reflecting the bright sky! This is due to the lack of another reflection probe for the scene.\nSo we could easily address this by adding another probe:\n  Use yet another reflection probe for interior reflections.   and bake!\n  A corrected interior look.   Details and Props It looks we are on the right track now, we could start to add more details for the scene!\nAfter enabling all props meshes, we begin with the brushed steel. It is like the fireplace glass, but we set the Metallic = 1 to simulate the metal. As for the painting back and the table wood, we could use the same material with the floor, with different base color textures. The sofa, painting, pot, leaves and dirt are just the Lambertians so HDRP/Lit works well.\nNote that we need to use Translucent material type for the leaves and alpha cutout the geometries according to the leaves map.\n  Material setup for the leaves.   Finally, we use a transparent surface type for the glass material.\n  Material setup for the glass.   Here comes another nasty stuff in realtime rendering \u0026ndash; refraction. Refraction is notoriously difficult to achieve in realtime rendering and is usually approximated by Screen Space Refraction:\n  Enable Screen Space Refraction in the volume setting.   Results and Discussions After adding up all props and setting up the materials correctly, we could simply bake our GI and here is what I get:\n  Final baked result for the living room in Unity!   It looks kind of cool \u0026ndash; perhaps not in a sense of realism but in a artistic style.\n  Path traced reference image rendered by my Colvillea renderer.   By comparing with the ground truth image from the ray tracing, we find that the glass seems to be weird and we lack all the nice occlusion from the objects. As for the user\u0026rsquo;s experience, I think it requires artists to pay more attention to the stuff behind rendering. For example, we have to consider the resolution or padding for the lightmap UVs, tuning the baking parameters, fixing light leaking due to the lack of reflection probes etc.\nHowever, the image generated still looks good and the performance in realtime framerates (~80 FPS) is appealing; or we have to wait for hours to get a noise-free image using offline ray tracing!\nHere is another result by leveraging the post-processing pipeline:\n  Featured image for the living-room after post processing.   References  Unity\u0026rsquo;s Blog for HDRP Unity\u0026rsquo;s Documentation for HDRP ArchViz with Unity\u0026rsquo;s HDRP Fontainebleau Demo \u0026ndash; A Unity\u0026rsquo;s Official Demo for HDRP Laurent Belcour\u0026rsquo;s Layered BSDF Paper  ","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"3e7c96beb89d8a0f21cc803d2163b960","permalink":"https://hearwindsaying.github.io/post/render-the-livingroom-in-unity/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/render-the-livingroom-in-unity/","section":"post","summary":"Notes This is my final project for a virtual reality and game development course. My contribution is bringing a Benedikt Bitterli\u0026rsquo;s famous rendering scene living-room to Unity and showing how we could use various rasterization techniques to approximate the beauty image produced by offline renderer.","tags":["Project"],"title":"Bring the Living-Room to Unity.","type":"post"}]